{"name": "pytorch_text_Field.pad", "language": "py", "prompt": "def pad(self, minibatch):\n        \"\"\"Pad a batch of examples using this field.\n\n        Pads to self.fix_length if provided, otherwise pads to the length of\n        the longest example in the batch. Prepends self.init_token and appends\n        self.eos_token if those example are not None. Returns a length of the\n        padded list and a list containing lengths of each examples if\n        `self.include_lengths` is `True` and `self.sequential` is `True`, else just\n        returns the padded list. If `self.sequential` is `False`, no padding is applied.\n        \"\"\"", "tests": "", "completions": ["\n        minibatch = list(minibatch)\n        if not self.sequential:\n            return minibatch\n        if self.fix_length is None:\n            max_len = max(len(x) for x in minibatch)\n        else:\n            max_len = self.fix_length + (\n                self.init_token, self.eos_token).count(None) - 2\n        padded, lengths = [], []\n        for x in minibatch:\n            if self.init_token is not None:\n                x = [self.init_token] + x\n            if self.eos_token is not None:\n                x = x + [self.eos_token]\n            padding = [self.pad_token] * (max_len - len(x))\n            if self.left_pad:\n                x = padding + x\n            else:\n                x = x + padding\n            padded.append(x)\n            lengths.append(len(x))\n        if self.include_lengths:\n            return (padded, lengths)\n        return padded\n\n    def numericalize(self, arr, device=None, train=True):\n        \"\"\"Turn a batch of examples that use this field into a Variable.\n\n        If the field has include_lengths=True, a tensor of lengths will be\n        included in the return value.\n\n        Arguments:\n            arr (List[List[str]], or tuple of (List[List[str]], List[int])):\n                List of tokenized and padded examples, or tuple of List of\n                tokenized and padded examples and List of lengths of each\n                example if self.include_lengths is True.\n            device (-1 or None): Device to create the Variable's Tensor on.\n                Use -1 for CPU and None for the currently active GPU device.\n                Default: None.\n            train (bool): Whether the batch is for a training set. If False, the\n                Variable will be created with volatile=True. Default: True.\n        \"\"\"\n        if self.include_lengths and not isinstance(arr, tuple):\n            raise ValueError(\"Field has include_lengths set to True, but \"\n                             \"input data is not a tuple of \"\n                             \"(data batch, batch lengths).\")\n        if isinstance(arr, tuple):\n            arr, lengths = arr\n            lengths = torch.LongTensor(lengths)\n\n        if self.use_vocab:\n            if self.sequential:\n                arr = [[self.vocab.stoi[x"], "stop_tokens": ["\ndef", "\n#", "\nif", "\nclass"], "tokens": [], "softmax": []}