{"name": "GPflow_GPflow_ExternalOptimizerInterface.minimize", "language": "py", "prompt": "def minimize(self,\n               session=None,\n               feed_dict=None,\n               fetches=None,\n               step_callback=None,\n               loss_callback=None,\n               **run_kwargs):\n    \"\"\"Minimize a scalar `Tensor`.\n\n    Variables subject to optimization are updated in-place at the end of\n    optimization.\n\n    Note that this method does *not* just return a minimization `Op`, unlike\n    `Optimizer.minimize()`; instead it actually performs minimization by\n    executing function to control a `Session`.\n\n    Args:\n      session: A `Session` instance.\n      feed_dict: A function functions to be passed to calls to `session.run`.\n      fetches: A list of `Tensor`s to fetch and supply to `loss_callback`\n        as positional arguments.\n      step_callback: A function to be called at each optimization step;\n        arguments are the current values of all optimization variables\n        flattened into a single vector.\n      loss_callback: A function to be called every time the loss and gradients\n        are computed, with evaluated fetches supplied as positional arguments.\n      **run_kwargs: kwargs to pass to `session.run`.\n    \"\"\"", "tests": "", "completions": ["\n    fetches = [self._opt_op] + fetches\n    loss_callback = functools.partial(\n        _call_loss_callback, loss_callback, fetches)\n    super(ProxyGradientDescentOptimizer, self).minimize(\n        session=session,\n        feed_dict=feed_dict,\n        fetches=fetches,\n        step_callback=step_callback,\n        loss_callback=loss_callback,\n        **run_kwargs)\n"], "stop_tokens": ["\ndef", "\n#", "\nif", "\nclass"], "tokens": [], "softmax": []}